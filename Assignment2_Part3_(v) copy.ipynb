{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5658f1dfe1584447",
   "metadata": {},
   "source": [
    "# Preservation of the best model in part (ii)\n",
    "\n",
    "use of the best parameters identified in the analysis (20 neurons, learning rate 0.1, batch size 32, random seed 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de928583739c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_data():\n",
    "    # Load the data\n",
    "    df = pd.read_csv('Body_Fat.csv')\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop('BodyFat', axis=1)\n",
    "    y = df['BodyFat']\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.25, random_state=42\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler, X.columns\n",
    "\n",
    "def create_best_model(input_shape):\n",
    "    \"\"\"Create the model with the best hyperparameters\"\"\"\n",
    "    tf.random.set_seed(123)  # Best seed from analysis\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(20, activation='sigmoid'),  # Best number of neurons\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)  # Best learning rate\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_save_model(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \"\"\"Train the best model and save it\"\"\"\n",
    "    model = create_best_model(X_train.shape[1])\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=100,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train model with best batch size\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20000,\n",
    "        batch_size=32,  # Best batch size\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    results = evaluate_model(model, X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \n",
    "    # Save model\n",
    "    model.save('best_full_model.keras')\n",
    "    \n",
    "    return model, history, results\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    sets = {\n",
    "        'train': (X_train, y_train),\n",
    "        'val': (X_val, y_val),\n",
    "        'test': (X_test, y_test)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for set_name, (X, y) in sets.items():\n",
    "        y_pred = model.predict(X, verbose=0)\n",
    "        mse = np.mean((y - y_pred.flatten()) ** 2)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        results[set_name] = {'mse': mse, 'r2': r2}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def sensitivity_analysis(model, X_train, feature_names, num_samples=1000):\n",
    "    \"\"\"Perform sensitivity analysis on the model\"\"\"\n",
    "    sensitivities = {}\n",
    "    \n",
    "    base_prediction = model.predict(X_train, verbose=0).flatten()\n",
    "    \n",
    "    for i, feature in enumerate(feature_names):\n",
    "        X_modified = X_train.copy()\n",
    "        \n",
    "        # Create variations of the feature\n",
    "        variations = np.linspace(0, 1, num_samples)\n",
    "        predictions = []\n",
    "        \n",
    "        for variation in variations:\n",
    "            X_modified[:, i] = variation\n",
    "            pred = model.predict(X_modified, verbose=0).flatten()\n",
    "            predictions.append(np.mean(pred))\n",
    "        \n",
    "        sensitivity = np.std(predictions)\n",
    "        sensitivities[feature] = sensitivity\n",
    "    \n",
    "    return sensitivities\n",
    "\n",
    "def main():\n",
    "    # Prepare data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, scaler, feature_names = prepare_data()\n",
    "    \n",
    "    # Train and save model\n",
    "    model, history, results = train_and_save_model(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    for set_name, metrics in results.items():\n",
    "        print(f\"{set_name.capitalize()} set - MSE: {metrics['mse']:.4f}, R²: {metrics['r2']:.4f}\")\n",
    "    \n",
    "    # Perform sensitivity analysis\n",
    "    sensitivities = sensitivity_analysis(model, X_train, feature_names)\n",
    "    \n",
    "    # Sort and print feature sensitivities\n",
    "    sorted_sensitivities = sorted(sensitivities.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\nFeature Sensitivities:\")\n",
    "    for feature, sensitivity in sorted_sensitivities:\n",
    "        print(f\"{feature}: {sensitivity:.4f}\")\n",
    "    \n",
    "    return model, history, results, sensitivities\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, history, results, sensitivities = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1057bac3c59cec43",
   "metadata": {},
   "source": [
    "# Preservation of the best model in part (iv)\n",
    "\n",
    "model structure using 5 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62221d0e957074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T02:57:10.320363Z",
     "start_time": "2024-10-06T02:47:41.036254Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the selected features based on strong correlations\n",
    "SELECTED_FEATURES = ['Density', 'Abdomen', 'Chest', 'Hip', 'Weight', \n",
    "                     'Thigh', 'Knee', 'Biceps', 'Neck']\n",
    "\n",
    "def prepare_data():\n",
    "    # Load the data\n",
    "    df = pd.read_csv('Body_Fat.csv')\n",
    "    \n",
    "    # Select features and target\n",
    "    X = df[SELECTED_FEATURES]\n",
    "    y = df['BodyFat']\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=SELECTED_FEATURES)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.25, random_state=42\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler\n",
    "\n",
    "def create_best_model(input_shape):\n",
    "    \"\"\"Create the model with the best hyperparameters\"\"\"\n",
    "    tf.random.set_seed(123)  # For reproducibility\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(5, activation='sigmoid'),  # Best number of neurons from analysis\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_save_model(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \"\"\"Train the best model and save it\"\"\"\n",
    "    model = create_best_model(X_train.shape[1])\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=100,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20000,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    results = evaluate_model(model, X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \n",
    "    # Save model\n",
    "    model.save('best_selected_features_model.keras')\n",
    "    \n",
    "    return model, history, results\n",
    "\n",
    "def evaluate_model(model, X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    datasets = {\n",
    "        'train': (X_train, y_train),\n",
    "        'val': (X_val, y_val),\n",
    "        'test': (X_test, y_test)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, (X, y) in datasets.items():\n",
    "        y_pred = model.predict(X, verbose=0)\n",
    "        mse = np.mean((y - y_pred.flatten()) ** 2)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        results[name] = {'mse': mse, 'r2': r2}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def sensitivity_analysis(model, X_train, num_samples=1000):\n",
    "    \"\"\"Perform sensitivity analysis on the model\"\"\"\n",
    "    sensitivities = {}\n",
    "    X_numpy = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "    \n",
    "    for i, feature in enumerate(SELECTED_FEATURES):\n",
    "        X_modified = X_numpy.copy()\n",
    "        \n",
    "        variations = np.linspace(0, 1, num_samples)\n",
    "        predictions = []\n",
    "        \n",
    "        for variation in variations:\n",
    "            X_modified[:, i] = variation\n",
    "            pred = model.predict(X_modified, verbose=0).flatten()\n",
    "            predictions.append(np.mean(pred))\n",
    "        \n",
    "        sensitivity = np.std(predictions)\n",
    "        sensitivities[feature] = sensitivity\n",
    "    \n",
    "    return sensitivities\n",
    "\n",
    "def predict_body_fat(model, scaler, feature_values):\n",
    "    \"\"\"Function to make predictions using the saved model\"\"\"\n",
    "    # Create a DataFrame with the input values\n",
    "    input_df = pd.DataFrame([feature_values], columns=SELECTED_FEATURES)\n",
    "    \n",
    "    # Scale the input values\n",
    "    scaled_values = pd.DataFrame(scaler.transform(input_df), columns=SELECTED_FEATURES)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(scaled_values, verbose=0)[0][0]\n",
    "    return prediction\n",
    "\n",
    "def main():\n",
    "    # Prepare data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, scaler = prepare_data()\n",
    "    \n",
    "    # Train and save model\n",
    "    model, history, results = train_and_save_model(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nModel Performance:\")\n",
    "    for dataset, metrics in results.items():\n",
    "        print(f\"{dataset.capitalize()} set - MSE: {metrics['mse']:.4f}, R²: {metrics['r2']:.4f}\")\n",
    "    \n",
    "    # Perform sensitivity analysis\n",
    "    sensitivities = sensitivity_analysis(model, X_train)\n",
    "    \n",
    "    # Sort and print feature sensitivities\n",
    "    sorted_sensitivities = sorted(sensitivities.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\nFeature Sensitivities (ordered by importance):\")\n",
    "    for feature, sensitivity in sorted_sensitivities:\n",
    "        print(f\"{feature}: {sensitivity:.4f}\")\n",
    "    \n",
    "    return model, history, results, sensitivities, scaler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, history, results, sensitivities, scaler = main()\n",
    "    \n",
    "    # Example of how to use the saved model for prediction\n",
    "    print(\"\\nExample Prediction:\")\n",
    "    sample_values = {\n",
    "        'Density': 1.0,\n",
    "        'Abdomen': 90,\n",
    "        'Chest': 100,\n",
    "        'Hip': 95,\n",
    "        'Weight': 70,\n",
    "        'Thigh': 55,\n",
    "        'Knee': 38,\n",
    "        'Biceps': 32,\n",
    "        'Neck': 37\n",
    "    }\n",
    "    \n",
    "    predicted_body_fat = predict_body_fat(model, scaler, sample_values)\n",
    "    print(f\"Predicted Body Fat: {predicted_body_fat:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc454354343a527f",
   "metadata": {},
   "source": [
    "# (v) Sensitivity Analysis\n",
    "\n",
    "## Comparison of Full and Reduced Models\n",
    "\n",
    "### Full Model (14 features):\n",
    "1. Top 3 most influential features: Density (ST: 0.966241), Abdomen (ST: 0.021510), Ankle (ST: 0.014437)\n",
    "2. Nonlinearity measure: 0.0046\n",
    "\n",
    "### Reduced Model (9 features):\n",
    "1. Top 3 most influential features: Density (ST: 0.985561), Weight (ST: 0.026100), Thigh (ST: 0.024532)\n",
    "2. Nonlinearity measure: 0.0168\n",
    "\n",
    "## Interpretation of Results\n",
    "\n",
    "1. **Feature Importance**: In both models, Density is by far the most influential feature, with a Total-order sensitivity index (ST) of over 0.96. This indicates that Density alone accounts for the vast majority of the variance in the body fat percentage predictions.\n",
    "\n",
    "2. **Nonlinear Relationships**: The difference between first-order (S1) and total-order (ST) indices indicates the presence of nonlinear relationships and interaction effects. The reduced model shows a higher degree of nonlinearity (0.0168) compared to the full model (0.0046), suggesting that the reduced model captures more complex relationships between the inputs and the output.\n",
    "\n",
    "3. **Interaction Effects**: \n",
    "   - Full model: The top interaction is between Density and Weight\n",
    "   - Reduced model: The top interaction is between Density and Thigh\n",
    "   These interactions suggest that these pairs of features have a combined effect on body fat percentage prediction that is greater than their individual effects.\n",
    "\n",
    "4. **Model Efficiency**: The reduced model successfully captures the most important features, as evidenced by the high ST value for Density and the presence of significant interaction effects.\n",
    "\n",
    "## Comparison with Correlation Analysis\n",
    "\n",
    "The sensitivity analysis reveals some differences from the correlation analysis:\n",
    "\n",
    "1. Correlation analysis top features: Abdomen, Chest, Hip\n",
    "2. Sensitivity analysis top features (full model): Density, Abdomen, Ankle\n",
    "3. Sensitivity analysis top features (reduced model): Density, Weight, Thigh\n",
    "\n",
    "This comparison highlights that sensitivity analysis provides a more comprehensive understanding of feature importance, especially for complex nonlinear relationships that neural networks can capture. While correlation analysis only captures linear relationships, sensitivity analysis accounts for both linear and nonlinear relationships, as well as interaction effects between features.\n",
    "\n",
    "## Proposed Improved Input Set\n",
    "\n",
    "Based on the sensitivity analysis results, we can propose an improved set of inputs for the smaller network:\n",
    "\n",
    "1. Retain: Density, Weight, Thigh, Knee, Neck, Biceps\n",
    "2. Add: Ankle, Age, Abdomen\n",
    "3. Consider removing: Chest, Hip\n",
    "\n",
    "Rationale:\n",
    "- Density remains the most crucial feature in both models.\n",
    "- Weight, Thigh, Knee, Neck, and Biceps show significant total-order effects in the reduced model.\n",
    "- Ankle, Age, and Abdomen show notable total-order effects in the full model but were not included in the reduced model.\n",
    "- Chest and Hip show relatively low total-order effects in both models and could potentially be removed to simplify the model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bafaab81efbe28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T04:05:16.056514Z",
     "start_time": "2024-10-06T04:05:12.161417Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from SALib.sample import sobol\n",
    "from SALib.analyze import sobol as sobol_analyze\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarning\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "def perform_sobol_analysis(model, feature_names, num_samples=1024):\n",
    "    feature_names = list(feature_names)\n",
    "    \n",
    "    problem = {\n",
    "        'num_vars': len(feature_names),\n",
    "        'names': feature_names,\n",
    "        'bounds': [[0, 1] for _ in range(len(feature_names))]\n",
    "    }\n",
    "    \n",
    "    param_values = sobol.sample(problem, num_samples)\n",
    "    Y = model.predict(param_values).flatten()\n",
    "    \n",
    "    Si = sobol_analyze.analyze(problem, Y)\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'S1': Si['S1'],\n",
    "        'ST': Si['ST'],\n",
    "    })\n",
    "    \n",
    "    # Add second-order interaction effects\n",
    "    S2 = Si['S2']\n",
    "    interaction_results = []\n",
    "    for i in range(len(feature_names)):\n",
    "        for j in range(i+1, len(feature_names)):\n",
    "            interaction_results.append({\n",
    "                'Features': f\"{feature_names[i]} × {feature_names[j]}\",\n",
    "                'S2': S2[i][j]\n",
    "            })\n",
    "    interaction_df = pd.DataFrame(interaction_results)\n",
    "    interaction_df = interaction_df.sort_values('S2', ascending=False)\n",
    "    \n",
    "    results_df = results_df.sort_values('ST', ascending=False)\n",
    "    \n",
    "    return results_df, interaction_df, Si\n",
    "\n",
    "def calculate_correlations(df, target='BodyFat'):\n",
    "    correlations = df.corr()[target].drop(target)\n",
    "    return correlations.sort_values(ascending=False)\n",
    "\n",
    "def visualize_sensitivity_results(full_results, reduced_results):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Sensitivity of the full model\n",
    "    plt.subplot(121)\n",
    "    plt.bar(full_results['Feature'], full_results['ST'])\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Full Model - Total Order Sensitivity')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Sensitivity of the reduced model\n",
    "    plt.subplot(122)\n",
    "    plt.bar(reduced_results['Feature'], reduced_results['ST'])\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Reduced Model - Total Order Sensitivity')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def visualize_interactions(full_interactions, reduced_interactions):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Interaction effects of the full model\n",
    "    plt.subplot(121)\n",
    "    top_full = full_interactions.nlargest(5, 'S2')\n",
    "    plt.bar(top_full['Features'], top_full['S2'])\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Full Model - Top 5 Interaction Effects')\n",
    "    \n",
    "    # Interaction effects of the reduced model\n",
    "    plt.subplot(122)\n",
    "    top_reduced = reduced_interactions.nlargest(5, 'S2')\n",
    "    plt.bar(top_reduced['Features'], top_reduced['S2'])\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Reduced Model - Top 5 Interaction Effects')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def interpret_sensitivity_results(full_results, reduced_results, full_interactions, reduced_interactions):\n",
    "    interpretation = \"\"\"\n",
    "    Sensitivity Analysis Interpretation:\n",
    "    \n",
    "    1. Comparison between models:\n",
    "    - Full model uses {} features, while reduced model uses {} features\n",
    "    - Top 3 most influential features in full model: {}\n",
    "    - Top 3 most influential features in reduced model: {}\n",
    "    \n",
    "    2. Nonlinear relationships:\n",
    "    - The difference between first-order (S1) and total-order (ST) indices \n",
    "      indicates the presence of nonlinear relationships\n",
    "    - Full model nonlinearity: {}\n",
    "    - Reduced model nonlinearity: {}\n",
    "    \n",
    "    3. Interaction effects:\n",
    "    - Top interaction in full model: {}\n",
    "    - Top interaction in reduced model: {}\n",
    "    \n",
    "    4. Model efficiency:\n",
    "    - The reduced model {} capture the most important features\n",
    "    \"\"\"\n",
    "    \n",
    "    full_nonlinearity = np.mean(full_results['ST'] - full_results['S1'])\n",
    "    reduced_nonlinearity = np.mean(reduced_results['ST'] - reduced_results['S1'])\n",
    "    \n",
    "    top_full_features = ', '.join(full_results['Feature'].head(3).tolist())\n",
    "    top_reduced_features = ', '.join(reduced_results['Feature'].head(3).tolist())\n",
    "    \n",
    "    effectiveness = \"successfully\" if reduced_nonlinearity >= full_nonlinearity * 0.8 else \"may not fully\"\n",
    "    \n",
    "    return interpretation.format(\n",
    "        len(full_results), len(reduced_results),\n",
    "        top_full_features, top_reduced_features,\n",
    "        f\"{full_nonlinearity:.4f}\", f\"{reduced_nonlinearity:.4f}\",\n",
    "        full_interactions['Features'].iloc[0], reduced_interactions['Features'].iloc[0],\n",
    "        effectiveness\n",
    "    )\n",
    "\n",
    "def compare_correlation_sensitivity(df, full_results, reduced_results):\n",
    "    correlations = calculate_correlations(df)\n",
    "    \n",
    "    comparison = \"\"\"\n",
    "    Correlation vs Sensitivity Analysis:\n",
    "    \n",
    "    1. Top features by correlation: {}\n",
    "    2. Top features by sensitivity (full model): {}\n",
    "    3. Top features by sensitivity (reduced model): {}\n",
    "    \n",
    "    Key differences:\n",
    "    - Correlation only captures linear relationships\n",
    "    - Sensitivity analysis captures both linear and nonlinear relationships\n",
    "    - Sensitivity analysis also accounts for interaction effects between features\n",
    "    \n",
    "    This comparison shows that sensitivity analysis provides a more comprehensive\n",
    "    understanding of feature importance, especially for complex nonlinear relationships\n",
    "    that neural networks can capture.\n",
    "    \"\"\"\n",
    "    \n",
    "    top_corr = ', '.join(correlations.head(3).index.tolist())\n",
    "    top_sens_full = ', '.join(full_results['Feature'].head(3).tolist())\n",
    "    top_sens_reduced = ', '.join(reduced_results['Feature'].head(3).tolist())\n",
    "    \n",
    "    return comparison.format(top_corr, top_sens_full, top_sens_reduced)\n",
    "\n",
    "def provide_recommendations(full_results, reduced_results):\n",
    "    full_important = set(full_results['Feature'].head(5).tolist())\n",
    "    reduced_important = set(reduced_results['Feature'].head(5).tolist())\n",
    "    \n",
    "    missing_important = full_important - reduced_important\n",
    "    \n",
    "    recommendations = \"\"\"\n",
    "    Recommendations for Input Selection:\n",
    "    \n",
    "    1. Current reduced model features seem {}\n",
    "    \n",
    "    2. Consider {}\n",
    "    \n",
    "    3. Additional features to consider: {}\n",
    "    \n",
    "    4. Features that could potentially be removed: {}\n",
    "    \n",
    "    These recommendations are based on the sensitivity analysis results,\n",
    "    taking into account both individual feature importance and interaction effects.\n",
    "    \"\"\"\n",
    "    \n",
    "    effectiveness = \"effective\" if len(missing_important) <= 1 else \"to need adjustment\"\n",
    "    action = \"keeping the current feature set\" if len(missing_important) <= 1 else \"adding some features from the full model\"\n",
    "    to_add = ', '.join(missing_important) if missing_important else \"None\"\n",
    "    to_remove = ', '.join(reduced_results['Feature'].tail(2).tolist()) if len(reduced_results) > 7 else \"None\"\n",
    "    \n",
    "    return recommendations.format(effectiveness, action, to_add, to_remove)\n",
    "\n",
    "def run_sensitivity_analysis():\n",
    "    # Load data\n",
    "    df = pd.read_csv('Body_Fat.csv')\n",
    "    \n",
    "    # Prepare full input features\n",
    "    X_full = df.drop('BodyFat', axis=1)\n",
    "    full_feature_names = X_full.columns.tolist()\n",
    "    \n",
    "    # Prepare reduced input features\n",
    "    selected_features = ['Density', 'Abdomen', 'Chest', 'Hip', 'Weight', \n",
    "                         'Thigh', 'Knee', 'Biceps', 'Neck']\n",
    "    X_reduced = df[selected_features]\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler_full = MinMaxScaler()\n",
    "    scaler_reduced = MinMaxScaler()\n",
    "    \n",
    "    X_full_scaled = scaler_full.fit_transform(X_full)\n",
    "    X_reduced_scaled = scaler_reduced.fit_transform(X_reduced)\n",
    "    \n",
    "    # Load models\n",
    "    try:\n",
    "        full_model = load_model('best_full_model.keras')\n",
    "        reduced_model = load_model('best_selected_features_model.keras')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models: {e}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Perform Sobol analysis\n",
    "    print(\"Performing Sobol analysis for full model...\")\n",
    "    full_results, full_interactions, full_Si = perform_sobol_analysis(full_model, full_feature_names)\n",
    "    \n",
    "    print(\"Performing Sobol analysis for reduced model...\")\n",
    "    reduced_results, reduced_interactions, reduced_Si = perform_sobol_analysis(reduced_model, selected_features)\n",
    "    \n",
    "    return full_results, reduced_results, full_interactions, reduced_interactions, df\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        results = run_sensitivity_analysis()\n",
    "        if results is None:\n",
    "            return\n",
    "        \n",
    "        full_results, reduced_results, full_interactions, reduced_interactions, df = results\n",
    "        \n",
    "        # 1. Display of basic results\n",
    "        print(\"\\nFull Model Sensitivity Results:\")\n",
    "        print(full_results)\n",
    "        print(\"\\nReduced Model Sensitivity Results:\")\n",
    "        print(reduced_results)\n",
    "        \n",
    "        # 2. Visualisation of results\n",
    "        visualize_sensitivity_results(full_results, reduced_results)\n",
    "        visualize_interactions(full_interactions, reduced_interactions)\n",
    "        \n",
    "        # 3. Interpretation of results\n",
    "        interpretation = interpret_sensitivity_results(\n",
    "            full_results, reduced_results, full_interactions, reduced_interactions\n",
    "        )\n",
    "        print(\"\\nInterpretation of Results:\")\n",
    "        print(interpretation)\n",
    "        \n",
    "        # 4. Comparison with correlation analysis\n",
    "        correlation_comparison = compare_correlation_sensitivity(df, full_results, reduced_results)\n",
    "        print(\"\\nComparison with Correlation Analysis:\")\n",
    "        print(correlation_comparison)\n",
    "        \n",
    "        # 5. Provision of advice\n",
    "        recommendations = provide_recommendations(full_results, reduced_results)\n",
    "        print(\"\\nRecommendations for Input Selection:\")\n",
    "        print(recommendations)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
